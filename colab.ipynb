{"cells":[{"cell_type":"markdown","metadata":{"id":"Uss5I9O4cgmh"},"source":["This notebook can be used in Google colab to train, test, and evaluate different versions of BLIP 2 on the NYTimes cartoon image captioning dataset."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4136,"status":"ok","timestamp":1713078429760,"user":{"displayName":"Jason Sheinkopf","userId":"09415621891383882097"},"user_tz":-540},"id":"WKNUni9OcoNB"},"outputs":[],"source":["import sys\n","import os\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASEke_X7crSo"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"pLqX1xgikslG"},"source":["Clone the repo to your Google Drive and change `project_dir` to match your file structure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REBXeKxBkQdv"},"outputs":[],"source":["project_dir = '/content/drive/MyDrive/Projects/Python_Projects/blip2cap'\n","os.chdir(project_dir)\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt5qVEvsci00"},"outputs":[],"source":["!pip install -r requirements.colab.txt -q;"]},{"cell_type":"markdown","metadata":{"id":"TRLup8A21QSY"},"source":["Use this cell to train or test with any model by passing arguments as follows.\n","TRAIN.ENABLE True\n","TRAIN.EPOCHS 5"]},{"cell_type":"code","source":["# running without a key will run but not push results to the website\n","os.environ[\"WANDB_API_KEY\"] = input(\"Please enter your WandB API key: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaaSpFKeJ6p2","executionInfo":{"status":"ok","timestamp":1713065245764,"user_tz":-540,"elapsed":7264,"user":{"displayName":"Jason Sheinkopf","userId":"09415621891383882097"}},"outputId":"ef16edad-5834-4ca6-c587-c792e17f981f"},"execution_count":33,"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter your WandB API key: 4db8b749fdf8d2d397c6ab1fde5efb823aa1b908\n"]}]},{"cell_type":"code","source":["!python tools/run_net.py TRAIN.ENABLE True TRAIN.EPOCHS 1 TEST.NUM_BATCHES 1;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQYGMUGXfSHe","executionInfo":{"status":"ok","timestamp":1713074228491,"user_tz":-540,"elapsed":143473,"user":{"displayName":"Jason Sheinkopf","userId":"09415621891383882097"}},"outputId":"85a5c165-0d0d-4728-ab31-b72ba8270b52"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-14 05:54:49.239196: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-14 05:54:49.239246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-14 05:54:49.240627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-14 05:54:50.793700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","DATA:\n","  ANNOTATION: explanation\n","  DATASET: mhessel/newyorker_caption_contest\n","  FEATURE: image_description\n","  PROCESSOR: Salesforce/blip2-opt-2.7b\n","MODEL:\n","  ARCH: opt_2_7\n","  CHECKPOINT_DIR: checkpoints\n","  CHECKPOINT_FILE_PATH: \n","  DEVICE: cuda\n","  DROPOUT_RATE: 0.5\n","  OUTPUT_DIR: \n","RNG: 42\n","TEST:\n","  BATCH_SIZE: 4\n","  DATASET: nytimes\n","  NUM_BATCHES: 1\n","TRAIN:\n","  AUTO_RESUME: True\n","  BATCH_SIZE: 4\n","  CHECKPOINT_PERIOD: 1\n","  ENABLE: True\n","  EPOCHS: 1\n","  EVAL_PERIOD: 2\n","  WANDB_ENTITY: captioneers\n","Running with config\n","DATA:\n","  ANNOTATION: explanation\n","  DATASET: mhessel/newyorker_caption_contest\n","  FEATURE: image_description\n","  PROCESSOR: Salesforce/blip2-opt-2.7b\n","MODEL:\n","  ARCH: opt_2_7\n","  CHECKPOINT_DIR: checkpoints\n","  CHECKPOINT_FILE_PATH: \n","  DEVICE: cuda\n","  DROPOUT_RATE: 0.5\n","  OUTPUT_DIR: \n","RNG: 42\n","TEST:\n","  BATCH_SIZE: 4\n","  DATASET: nytimes\n","  NUM_BATCHES: 1\n","TRAIN:\n","  AUTO_RESUME: True\n","  BATCH_SIZE: 4\n","  CHECKPOINT_PERIOD: 1\n","  ENABLE: True\n","  EPOCHS: 1\n","  EVAL_PERIOD: 2\n","  WANDB_ENTITY: captioneers\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonsheinkopf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonsheinkopf\u001b[0m (\u001b[33mcaptioneers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Projects/Python_Projects/blip2cap/wandb/run-20240414_055500-q2240roa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mopt_2_7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/captioneers/blip2cap\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/captioneers/blip2cap/runs/q2240roa\u001b[0m\n","Loading checkpoint shards: 100% 2/2 [01:19<00:00, 39.64s/it]\n","trainable params: 5,242,880 || all params: 3,749,922,816 || trainable%: 0.13981301102065136\n","Training with config:\n","DATA:\n","  ANNOTATION: explanation\n","  DATASET: mhessel/newyorker_caption_contest\n","  FEATURE: image_description\n","  PROCESSOR: Salesforce/blip2-opt-2.7b\n","MODEL:\n","  ARCH: opt_2_7\n","  CHECKPOINT_DIR: checkpoints\n","  CHECKPOINT_FILE_PATH: \n","  DEVICE: cuda\n","  DROPOUT_RATE: 0.5\n","  OUTPUT_DIR: \n","RNG: 42\n","TEST:\n","  BATCH_SIZE: 4\n","  DATASET: nytimes\n","  NUM_BATCHES: 1\n","TRAIN:\n","  AUTO_RESUME: True\n","  BATCH_SIZE: 4\n","  CHECKPOINT_PERIOD: 1\n","  ENABLE: True\n","  EPOCHS: 1\n","  EVAL_PERIOD: 2\n","  WANDB_ENTITY: captioneers\n","Start epoch: 0\n","0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(\n","11it [00:21,  1.97s/it]\n","Epoch 0 training loss: 0.08881209935897436\n","len(input_ids)=4 | len(gen_ids)=4\n","<built-in method type of Tensor object at 0x7acce603b0b0> | fgen_ids.type=<built-in method type of Tensor object at 0x7acced26a250>\n","torch.Size([4, 50]) | fgen_ids.shape=torch.Size([4, 17])\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Projects/Python_Projects/blip2cap/tools/run_net.py\", line 125, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/Projects/Python_Projects/blip2cap/tools/run_net.py\", line 104, in main\n","    train(cfg, model, train_loader, test_loader, processor, wandb)\n","  File \"/content/drive/MyDrive/Projects/Python_Projects/blip2cap/tools/train_net.py\", line 108, in train\n","    test_loss, metrics = test(cfg, model, test_loader, processor)\n","  File \"/content/drive/MyDrive/Projects/Python_Projects/blip2cap/tools/test_net.py\", line 71, in test\n","    test_loss, metrics = perform_test(test_loader, model, cfg, processor)\n","  File \"/content/drive/MyDrive/Projects/Python_Projects/blip2cap/tools/test_net.py\", line 40, in perform_test\n","    outputs = model(input_ids=gen_ids,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 563, in forward\n","    return self.get_base_model()(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 166, in new_forward\n","    output = module._old_forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/modeling_blip_2.py\", line 1746, in forward\n","    loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3059, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","ValueError: Expected input batch_size (192) to match target batch_size (196).\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 3.78711\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mopt_2_7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/captioneers/blip2cap/runs/q2240roa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/captioneers/blip2cap\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240414_055500-q2240roa/logs\u001b[0m\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}